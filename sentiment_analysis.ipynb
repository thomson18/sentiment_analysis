{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b560e0",
   "metadata": {},
   "source": [
    "### Importing the libraries:\n",
    "We import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install langdetect\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c01af948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "from langdetect import detect\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5168bad",
   "metadata": {},
   "source": [
    "### Loading the dataset:\n",
    "We use the pandas library to load the csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "584df8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentiment_analysis.csv', encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6056e9b",
   "metadata": {},
   "source": [
    "### Initial Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "be984472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f39086cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d9e5e",
   "metadata": {},
   "source": [
    "### Pre-processing steps and Transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b726720",
   "metadata": {},
   "source": [
    "The columns' names for this dataset are not clear, so to fix it, we change the columns' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2d75e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the column names\n",
    "data.columns = ['sentiment', 'id', 'date', 'flag', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1f92ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns we don't need\n",
    "data = data.drop(columns= ['flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a692da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "id           0\n",
       "date         0\n",
       "user         0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1ffe1",
   "metadata": {},
   "source": [
    "This indicates that there are no null values in any of the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5cb687e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing contracted words into their root form.\n",
    "def contractions(text):\n",
    "    text = re.sub(r\"won't\", 'will not',text)\n",
    "    text = re.sub(r\"would't\", 'would not',text)\n",
    "    text = re.sub(r\"could't\", 'could not',text)\n",
    "    text = re.sub(r'\\'d', 'would',text)\n",
    "    text = re.sub(r\"can\\'t\", 'can not',text)\n",
    "    text = re.sub(r\"isn\\'t\", 'is not',text)\n",
    "    text = re.sub(r\"don\\'t\", 'do not',text)\n",
    "    text = re.sub(r\"n\\'t\", ' not', text)\n",
    "    text = re.sub(r\"\\'re\", ' are', text)\n",
    "    text = re.sub(r\"\\'s\", ' is', text)\n",
    "    text = re.sub(r\"\\'ll\", ' will', text)\n",
    "    text = re.sub(r\"\\'t\", ' not', text)\n",
    "    text = re.sub(r\"\\'ve\", ' have', text)\n",
    "    text = re.sub(r\"\\'m\", ' am', text)\n",
    "    text = re.sub(r\"shoulda\", \"should have\", text)\n",
    "    text = re.sub(r\"gonna\", \"going to\", text)\n",
    "    text = re.sub(r\"wanna\", \"want to\", text)\n",
    "    text = re.sub(r\"wutcha\", \"what are you\", text)\n",
    "    text = re.sub(r\"^im\", \"I am\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaea3b6",
   "metadata": {},
   "source": [
    "Defining the pre-processing steps and Transformations for the tweets in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "34c5254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the tweet text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Perform contractions\n",
    "    text = contractions(text)\n",
    "    \n",
    "    # Remove URLs from the tweet text\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\w+.com', '', text)\n",
    "    \n",
    "    # Remove mentions from the tweet text\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    \n",
    "    #Remove numbers from the tweet text\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    \n",
    "    #Remove one or two letter words from the tweet text\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \" \", text)\n",
    "    \n",
    "    # Remove hashtags and tabs from the tweet text\n",
    "    text = re.sub(r'#|\\t\\s|\\t', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove punctuation from the tweet text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stop words from the tweet text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenizing and filtering the tweet text\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Spell correction\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    for word in filtered_text:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_text.append(corrected_word)\n",
    "            \n",
    "    # Removing non-English words\n",
    "    english_words = set(words.words())\n",
    "    english_text = [word for word in corrected_text if len(word) > 2 and word in english_words]\n",
    "    \n",
    "    # Lemmatize each word in the tweet text to reduce each word to its base form \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in english_text]\n",
    "    \n",
    "    # Join the words back into a single string and return\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae46531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the preprocess_text function to every tweet in the text column.\n",
    "data['text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7660a87",
   "metadata": {},
   "source": [
    "### Imputing values:\n",
    "We have 0 and 4 as values in sentiment column. We change the value 4 to 1. Here 0 indicates 'Negative' and 1 indicates 'Positive'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcb3ba",
   "metadata": {},
   "source": [
    "### Cleansed Dataframe:\n",
    "After performing pre-processing, transformations, and imputing, this is our cleansed dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32130aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cc51f",
   "metadata": {},
   "source": [
    "### About the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying shape\n",
    "print('Shape of Data Frame:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec6d8e",
   "metadata": {},
   "source": [
    "### Highlighting features/predictors/target variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc329d92",
   "metadata": {},
   "source": [
    "Features/Predictors:\n",
    "- text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9ea17",
   "metadata": {},
   "source": [
    "Target Variable:\n",
    "- sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ecd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information about the dataframe\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3bd21",
   "metadata": {},
   "source": [
    "### Displaying descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c555ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a01440",
   "metadata": {},
   "source": [
    "## Visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d748a",
   "metadata": {},
   "source": [
    "### Pie Chart of Sentiment Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9629fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences for each sentiment\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Plot the sentiment distribution as a pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sentiment_counts, labels=['Negative', 'Positive'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b1926",
   "metadata": {},
   "source": [
    "We can observe that the dataframe has equal positive and negative sentiment texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e34b55",
   "metadata": {},
   "source": [
    "### Word cloud of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stop_words,\n",
    "                      min_font_size = 10).generate(''.join(data['text'])+\" \")\n",
    "plt.figure(figsize = (8, 8)) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17e500",
   "metadata": {},
   "source": [
    "### Positive and Negative Sentiment Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a65cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a WordCloud object for positive sentiment\n",
    "positive_text = data[data['sentiment'] == 1]['text'].values\n",
    "positive_text = ' '.join(positive_text)\n",
    "positive_wordcloud = WordCloud(background_color='white').generate(positive_text)\n",
    "\n",
    "# Create a WordCloud object for negative sentiment\n",
    "negative_text = data[data['sentiment'] == 0]['text'].values\n",
    "negative_text = ' '.join(negative_text)\n",
    "negative_wordcloud = WordCloud(background_color='white').generate(negative_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive Sentiment Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative Sentiment Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ad6a5",
   "metadata": {},
   "source": [
    "### Top 10 Active Users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb782d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = data['user'].value_counts().head(10)\n",
    "\n",
    "# Plot the user activity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(user_counts.index, user_counts.values)\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 Active Users')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209ec52",
   "metadata": {},
   "source": [
    "Code to calculate number of occurences of different words in all of the tweet texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words using NLTK\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "data['tokens'] = data['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "# Create a list of all the words in the dataset\n",
    "all_words = [word for tokens in data['tokens'] for word in tokens]\n",
    "\n",
    "# Count the frequency of each word using Counter\n",
    "val_count = Counter(all_words)\n",
    "\n",
    "# Visualise the top 10 occuring words in the tweet texts\n",
    "val_counts= pd.DataFrame.from_dict(val_count, orient= 'index').reset_index()\n",
    "val_counts= val_counts.rename(columns= {'index': 'Word', 0: 'Occurrence'})\n",
    "val_counts.sort_values('Occurrence', ascending= False, inplace= True)\n",
    "val_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af3cc6",
   "metadata": {},
   "source": [
    "### Bar Plot of Top 30 Occurring words in the Tweet Texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a12b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30 = val_counts.iloc[:30, :]\n",
    "\n",
    "plt.figure(figsize= (20,8))\n",
    "plt.bar(x= top_30.Word, height= top_30.Occurrence)\n",
    "plt.title('Top 30 Occurring Words in the Tweet texts')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Occurrence')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbb6d5",
   "metadata": {},
   "source": [
    "### Splitting the dataset for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0d152",
   "metadata": {},
   "source": [
    "### Vectorizing the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text into a vector of numerical values.\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=650, max_df=0.9, min_df=7, binary=True, \n",
    "                                   ngram_range=(1,2))\n",
    "\n",
    "# Fit the vectorizer to the text data\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "# Transform the text data using the fitted vectorizer\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b392c79",
   "metadata": {},
   "source": [
    "### Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Multinomial Naive Bayes (MNB) algorithm for model building.\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b616120",
   "metadata": {},
   "source": [
    "### Making initial prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = vectorizer.transform(X_test)\n",
    "y_pred_nb = model_nb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507e8e3",
   "metadata": {},
   "source": [
    "### Displaying initial prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initial Prediction:')\n",
    "print(y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afe693",
   "metadata": {},
   "source": [
    "### Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the model using different metrics.\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Confusion matrix for Naive Bayes model\n",
    "conf_mat_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "cm_display_nb = ConfusionMatrixDisplay(confusion_matrix = conf_mat_nb, display_labels = [False, True])\n",
    "\n",
    "cm_display_nb.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate different models.\n",
    "def metrics(y_test, y_pred):\n",
    "    metrics = []\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = recall_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    metrics.append(accuracy)\n",
    "    metrics.append(precision)\n",
    "    metrics.append(recall)\n",
    "    metrics.append(f1)\n",
    "    metrics.append(rmse)\n",
    "    metrics.append(r2)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_metrics = metrics(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bc91c",
   "metadata": {},
   "source": [
    "### Building different models:\n",
    "\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92871f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression object\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_lr = model_lr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of the model using different metrics.\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79345dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix for Logistic regression model\n",
    "conf_mat_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "cm_display_lr = ConfusionMatrixDisplay(confusion_matrix = conf_mat_lr, display_labels = [False, True])\n",
    "\n",
    "cm_display_lr.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metrics = metrics(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their potential values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create a LogisticRegression object\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model_lr, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_lr_tuned = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate evaluation metrics using the best model\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_lr_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_lr_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_lr_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_lr_tuned)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy (Tuned):\", accuracy_tuned)\n",
    "print(\"Precision (Tuned):\", precision_tuned)\n",
    "print(\"Recall (Tuned):\", recall_tuned)\n",
    "print(\"F1 Score (Tuned):\", f1_tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb916fb",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Create an instance of DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "dt_model = dt.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dt = dt_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20612807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c029399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Confusion matrix for Decision Tree model\n",
    "conf_mat_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object for visualization\n",
    "cm_display_dt = ConfusionMatrixDisplay(confusion_matrix=conf_mat_dt, display_labels=[False, True])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm_display_dt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_metrics = metrics(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a62239",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Create an instance of XGB Classifier\n",
    "xg = xgb.XGBClassifier(learning_rate=0.50, max_depth=10, random_state=42) #based on the tuned parameters\n",
    "\n",
    "# Train the XGBoost model\n",
    "xg_model = xg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_xg  = xg_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63bde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report.\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, y_pred_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Confusion matrix for Naive Bayes model\n",
    "conf_mat_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "\n",
    "cm_display_xg = ConfusionMatrixDisplay(confusion_matrix = conf_mat_xg, display_labels = [False, True])\n",
    "\n",
    "cm_display_xg.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaefb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_metrics = metrics(y_test, y_pred_xg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bbcfa",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "748041eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36352c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 \n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac578148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df7d2f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bfb6ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 18528s 926ms/step - loss: 0.4880 - accuracy: 0.7614\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 17098s 855ms/step - loss: 0.4678 - accuracy: 0.7734\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 17215s 861ms/step - loss: 0.4579 - accuracy: 0.7795\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 16833s 842ms/step - loss: 0.4497 - accuracy: 0.7843\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 16834s 842ms/step - loss: 0.4423 - accuracy: 0.7887\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 17370s 869ms/step - loss: 0.4347 - accuracy: 0.7933\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 17008s 850ms/step - loss: 0.4270 - accuracy: 0.7981\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 19481s 974ms/step - loss: 0.4192 - accuracy: 0.8028\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 17275s 864ms/step - loss: 0.4113 - accuracy: 0.8072\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 17331s 867ms/step - loss: 0.4036 - accuracy: 0.8116\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 17148s 857ms/step - loss: 0.3964 - accuracy: 0.8158\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 17176s 859ms/step - loss: 0.3896 - accuracy: 0.8196\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 17232s 862ms/step - loss: 0.3836 - accuracy: 0.8226\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 17413s 871ms/step - loss: 0.3780 - accuracy: 0.8257\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 17895s 895ms/step - loss: 0.3730 - accuracy: 0.8283\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 17940s 897ms/step - loss: 0.3684 - accuracy: 0.8305\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 17880s 894ms/step - loss: 0.3645 - accuracy: 0.8325\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 17720s 886ms/step - loss: 0.3607 - accuracy: 0.8348\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 17867s 893ms/step - loss: 0.3577 - accuracy: 0.8362\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 17833s 892ms/step - loss: 0.3547 - accuracy: 0.8373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9aed71460>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "model.fit(train_padded, y_train, epochs = 20, batch_size=batch_size, verbose = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a82a2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "save_model(model, \"lstm_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebf2f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# load model\n",
    "model = load_model('lstm_model_1.h5')\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25f6b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2988s 299ms/step - loss: 0.5368 - accuracy: 0.7595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5368435382843018, 0.7595124840736389]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62ff1b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2983s 298ms/step\n",
      "Accuracy of prediction on test set :  0.7595125\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(y_test,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10fa9707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751553580461016\n",
      "0.7775970991738627\n",
      "0.7775970991738627\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, pred_labels)\n",
    "recall = recall_score(y_test, pred_labels)\n",
    "f1 = recall_score(y_test, pred_labels)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eba3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_metrics = metrics(y_test, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9c63a",
   "metadata": {},
   "source": [
    "### Metrics table of all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc48342b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>XG Boost</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.707156</td>\n",
       "      <td>0.720050</td>\n",
       "      <td>0.722956</td>\n",
       "      <td>0.759513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.720574</td>\n",
       "      <td>0.700742</td>\n",
       "      <td>0.695744</td>\n",
       "      <td>0.751554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.679757</td>\n",
       "      <td>0.771224</td>\n",
       "      <td>0.795572</td>\n",
       "      <td>0.777597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1Score</td>\n",
       "      <td>0.679757</td>\n",
       "      <td>0.771224</td>\n",
       "      <td>0.795572</td>\n",
       "      <td>0.777597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric  Naive Bayes  Logistic Regression  XG Boost      LSTM\n",
       "0   Accuracy     0.707156             0.720050  0.722956  0.759513\n",
       "1  Precision     0.720574             0.700742  0.695744  0.751554\n",
       "2     Recall     0.679757             0.771224  0.795572  0.777597\n",
       "3    F1Score     0.679757             0.771224  0.795572  0.777597"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table which contain all the metrics of both the models.\n",
    "metrics_table = {\n",
    "        'Metric': ['Accuracy','Precision','Recall', 'F1Score', 'RMSE', 'R2'], \n",
    "        'Naive Bayes': nb_metrics,\n",
    "        'Logistic Regression': lr_metrics, 'Decision Tree': dt_metrics,\n",
    "        'XGBoost': xg_metrics,\n",
    "        'LSTM' : lstm_metrics\n",
    "        }\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_table, columns = ['Metric', 'Naive Bayes', 'Logistic Regression','Decision Tree',\n",
    "                                                    'XGBoost', 'LSTM'])\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c0f04",
   "metadata": {},
   "source": [
    "We can observe here that LSTM model has the highest accuracy followed by XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68c752",
   "metadata": {},
   "source": [
    "\n",
    "### Test sentences for prediction using LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8988185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie plot is terrible but it had good acting\n",
      "Predicted sentiment:  Positive\n",
      "I hope your day rocks as much as you do\n",
      "Predicted sentiment:  Positive\n",
      "I hate everyone\n",
      "Predicted sentiment:  Negative\n",
      "I love you, but I don't love you\n",
      "Predicted sentiment:  Positive\n",
      "you are too good for this world\n",
      "Predicted sentiment:  Positive\n",
      "you look too innocent, but I dont believe you\n",
      "Predicted sentiment:  Negative\n"
     ]
    }
   ],
   "source": [
    "sents = ['the movie plot is terrible but it had good acting', 'I hope your day rocks as much as you do', \n",
    "         'I hate everyone', \"I love you, but I don't love you\", 'you are too good for this world', \n",
    "         'you look too innocent, but I dont believe you']\n",
    "\n",
    "# Vectorizing the test sentences.\n",
    "v = tfidf.transform(sents)\n",
    "\n",
    "# Predicting sentiment on the test sentences.\n",
    "prediction = xg_model.predict(v)\n",
    "pred_labels = []\n",
    "\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    print(sents[i])\n",
    "    if pred_labels[i] == 1:\n",
    "        s = 'Positive'\n",
    "    else:\n",
    "        s = 'Negative'\n",
    "    print(\"Predicted sentiment: \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47f68f",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92352853",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/docs/reference/frame.html\n",
    "\n",
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://medium.com/analytics-vidhya/sentiment-analysis-on-amazon-reviews-using-tf-idf-approach-c5ab4c36e7a1\n",
    "\n",
    "https://towardsdatascience.com/nlp-preprocessing-with-nltk-3c04ee00edc0\n",
    "\n",
    "https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "\n",
    "https://www.w3schools.com/python/python_ml_confusion_matrix.asp\n",
    "\n",
    "https://scikit-learn.org/\n",
    "\n",
    "https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22289dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
